{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9oq3u7jA3Us"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gTQIIqIP_Lwq"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NhHwV3Dr_j9-"
   },
   "source": [
    "#### some low level function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9gqomcqMhoT7"
   },
   "outputs": [],
   "source": [
    "def Myrelu(X):\n",
    "    return tf.maximum(X, tf.zeros_like(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zBiQLCm_UttX"
   },
   "outputs": [],
   "source": [
    "def mySoftMax(X):\n",
    "    X_centered = X - tf.reduce_max(X) # to avoid overflow\n",
    "    X_exp = tf.exp(X_centered)\n",
    "    exp_sum = tf.reduce_sum(X_exp, axis=1)\n",
    "    return tf.transpose(tf.transpose(X_exp) / exp_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SR8E83R0_yMk"
   },
   "outputs": [],
   "source": [
    "def Mysigmoid(X):\n",
    "    return 1.0/(1.0 + tf.math.exp(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7BbEFaUnfmS"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    return -tf.reduce_mean(tf.log(tf.reduce_sum(y * t, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bHYTalu8_YYT"
   },
   "source": [
    "## 1. Learning the XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9lxQyRd_agM"
   },
   "outputs": [],
   "source": [
    "# xor task\n",
    "xor_data = np.array([[1, 0],\n",
    "                    [0, 1],\n",
    "                    [1, 1],\n",
    "                    [0, 0]])\n",
    "xor_label = np.array([[1], [1], [0], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H18HhOgz_-kW"
   },
   "outputs": [],
   "source": [
    "learning_rates = 0.01\n",
    "with tf.Graph().as_default() as g:\n",
    "    with tf.name_scope('data') as scope:\n",
    "        X = tf.placeholder(tf.float32, shape=[4,2], name = 'X')\n",
    "        Y = tf.placeholder(tf.float32, shape=[4,1], name = 'Y')\n",
    "    \n",
    "    with tf.name_scope('relu') as scope:\n",
    "        W1 = tf.Variable(tf.truncated_normal([2,2]), name = 'W1')\n",
    "        b1 = tf.Variable(tf.zeros([4,2]), name = 'b1')\n",
    "        relu = Myrelu(tf.add(tf.matmul(X, W1), b1))\n",
    "        \n",
    "    with tf.name_scope('sigmoid') as scope:\n",
    "        W2 = tf.Variable(tf.truncated_normal([2,1]), name = \"W2\")\n",
    "        b2 = tf.Variable(tf.zeros([4,1]), name = 'b2')\n",
    "        prediction = Mysigmoid(tf.add(tf.matmul(relu, W2), b2))\n",
    "        \n",
    "    with tf.name_scope(\"loss\") as scope:\n",
    "        loss = tf.reduce_mean(tf.squared_difference(prediction, Y)) \n",
    "\n",
    "    with tf.name_scope('optimizer') as scope:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rates).minimize(loss)\n",
    "        \n",
    "    with tf.name_scope(\"accuracy\") as scope:\n",
    "        t_predict = tf.argmax(prediction, axis=1)\n",
    "        t_actual = tf.argmax(Y, axis=1)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(t_predict, t_actual), tf.float32))\n",
    "        tf.summary.scalar('Accuracy', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "u_ThZpvbABv7",
    "outputId": "1ed0f836-9679-480e-948e-fbe1df1681ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "w of relu: \n",
      "     [-1.1005414 -1.6694647]\n",
      "     [-0.05850164 -0.5328449 ]\n",
      "bias of relu: \n",
      "     [0. 0.]\n",
      "     [0. 0.]\n",
      "     [0. 0.]\n",
      "     [-0.00088206 -0.00045285]\n",
      "w of sigmoid: \n",
      "     [1.4112934]\n",
      "     [0.72456217]\n",
      "bias of sigmoid: \n",
      "     [2.4099236]\n",
      "     [2.4099236]\n",
      "     [-2.4099236]\n",
      "     [-2.4099236]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for epoch in range(20001):\n",
    "        sess.run(optimizer, feed_dict={X: xor_data, Y: xor_label})\n",
    "    result = sess.run(prediction, feed_dict={X: xor_data, Y: xor_label})\n",
    "    print(\"Accuracy: \", sess.run(acc, feed_dict={X:xor_data, Y:xor_label}))\n",
    "    \n",
    "    print('w of relu: ')\n",
    "    for element in sess.run(W1):\n",
    "        print('    ', element)\n",
    "    print('bias of relu: ')\n",
    "    for element in sess.run(b1):\n",
    "        print('    ', element)\n",
    "    print('w of sigmoid: ')\n",
    "    for element in sess.run(W2):\n",
    "        print('    ', element)\n",
    "    print('bias of sigmoid: ')\n",
    "    for element in sess.run(b2):\n",
    "        print('    ', element)\n",
    "        \n",
    "    graph_dir = \"graphs/demo\"\n",
    "    # create a writer \n",
    "    writer = tf.summary.FileWriter(graph_dir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1euG6942AD5y"
   },
   "source": [
    "## 2. MNIST - Digits classification\n",
    "### First solution using fully *connected*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "colab_type": "code",
    "id": "EAyNSRdjAOUx",
    "outputId": "6556ebf7-5502-4dd5-f725-501618dfa5e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-e70db4c1b96c>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# load mnist data\n",
    "mnist = input_data.read_data_sets(\"data/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "CLORtWitAQfl",
    "outputId": "7546d13f-0a80-43b7-fb08-a368894e2fdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: 55000\n",
      "validation data size: 5000\n",
      "testing data size: 10000\n",
      "Shape of image: (784,)\n",
      "Shape of label: (10,)\n"
     ]
    }
   ],
   "source": [
    "print(\"training data size: {}\".format(mnist.train.num_examples))\n",
    "print(\"validation data size: {}\".format(mnist.validation.num_examples))\n",
    "print(\"testing data size: {}\".format(mnist.test.num_examples))\n",
    "\n",
    "print(\"Shape of image: {}\".format(mnist.train.images[0].shape))\n",
    "print(\"Shape of label: {}\".format(mnist.train.labels[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "DjzjtWSXASRC",
    "outputId": "f302c5a4-d244-4844-bcd1-4479aa9ed372"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/YAAADGCAYAAAB4ihtkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6f0e172320>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 5\n",
    "plt.figure(figsize=(18,18))\n",
    "for i in range(5):\n",
    "    plt.subplot(n, n, i+1)\n",
    "    plt.imshow(mnist.train.images[i].reshape(28,28), cmap='gray')\n",
    "    plt.title(np.argmax(mnist.train.labels[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xi9AyQlNAU1R"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "learning_rates = 0.5\n",
    "epochs = 10\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "5D5YzlcAAXMG",
    "outputId": "30774433-bf1b-407a-cb77-bc4299ca2e24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.191578761\n",
      "Epoch: 0002 cost= 0.051231673\n",
      "Epoch: 0003 cost= 0.029451991\n",
      "Epoch: 0004 cost= 0.019760672\n",
      "Epoch: 0005 cost= 0.014299860\n",
      "Epoch: 0006 cost= 0.010624113\n",
      "Epoch: 0007 cost= 0.008401911\n",
      "Epoch: 0008 cost= 0.006735386\n",
      "Epoch: 0009 cost= 0.005512852\n",
      "Epoch: 0010 cost= 0.004623068\n",
      "---\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    with tf.name_scope('data') as scope:\n",
    "        x = tf.placeholder(tf.float32, [None, 784])\n",
    "        y = tf.placeholder(tf.float32, [None, 10])\n",
    "        \n",
    "    with tf.name_scope('relu'):\n",
    "        W1 = tf.Variable(tf.random_normal([784, 400], stddev=0.03), name='W1')\n",
    "        b1 = tf.Variable(tf.zeros([400]), name='b1')\n",
    "        relu = Myrelu(tf.add(tf.matmul(x, W1), b1))\n",
    "        \n",
    "    with tf.name_scope('softmax'):\n",
    "        W2 = tf.Variable(tf.random_normal([400, 10], stddev=0.03), name='W2')\n",
    "        b2 = tf.Variable(tf.zeros([10]), name='b2')\n",
    "        prediction = mySoftMax(tf.add(tf.matmul(relu, W2), b2))\n",
    "        \n",
    "    with tf.name_scope(\"loss\") as scope:\n",
    "        loss = cross_entropy_error(tf.clip_by_value(prediction, 1e-10, 0.9999999), y) \n",
    "\n",
    "    with tf.name_scope('optimizer') as scope:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rates).minimize(loss)\n",
    "        \n",
    "    with tf.name_scope(\"accuracy\") as scope:\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        #t_predict = tf.argmax(prediction, axis=1)\n",
    "        #t_actual = tf.argmax(y, axis=1)\n",
    "        #acc = tf.reduce_mean(tf.cast(tf.equal(t_predict, t_actual), tf.float32))\n",
    "        #tf.summary.scalar('Accuracy', acc)\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    num_batch = int(mnist.train.num_examples/batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for _ in range(num_batch):\n",
    "            train_X, train_ys = mnist.train.next_batch(batch_size)\n",
    "            optimizer.run(feed_dict={x:train_X, y:train_ys})\n",
    "            avg_cost += loss.eval(feed_dict={x:train_X, y:train_ys}) / num_batch\n",
    "\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost), flush=True)\n",
    "\n",
    "    print(\"---\")\n",
    "    # 準確率\n",
    "    test_x = mnist.test.images[:batch_size]\n",
    "    test_t = mnist.test.labels[:batch_size]\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={x:test_x, y:test_t}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "032Zi9goAatr"
   },
   "source": [
    "## 2. MNIST - Digits classification\n",
    "### Second solution using cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "xbYXYdvUGjIn",
    "outputId": "79b6bdef-21da-47b5-c22b-dd1b68f87a18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-9f78d657411d>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 讀入 MNIST\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True, reshape=False)\n",
    "# 設定參數\n",
    "learning_rate = 0.1\n",
    "training_epochs = 2\n",
    "batch_size = 100\n",
    "example_X, example_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2mCoOpx2q6C8",
    "outputId": "d0bb41d3-4eb8-4ccb-da66-6334a349c6ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_X[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gu-xnBMAdz9r"
   },
   "outputs": [],
   "source": [
    "def flatten(X, window_h, window_w, window_c, out_h, out_w, stride=1, padding=0):\n",
    "    \n",
    "    X_padded = tf.pad(X, [[0,0], [padding, padding], [padding, padding], [0,0]])\n",
    "\n",
    "    windows = []\n",
    "    for y in range(out_h):\n",
    "        for x in range(out_w):\n",
    "            window = tf.slice(X_padded, [0, y*stride, x*stride, 0], [-1, window_h, window_w, -1])\n",
    "            windows.append(window)\n",
    "    stacked = tf.stack(windows) # shape : [out_h, out_w, n, filter_h, filter_w, c]\n",
    "\n",
    "    return tf.reshape(stacked, [-1, window_c*window_w*window_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yn-AR_Gec8wv"
   },
   "outputs": [],
   "source": [
    "def convolution(X, W, b, padding, stride):\n",
    "    n, h, w, c = map(lambda d: d.value, X.get_shape())\n",
    "    filter_h, filter_w, filter_c, filter_n = [d.value for d in W.get_shape()]\n",
    "    \n",
    "    out_h = (h + 2*padding - filter_h)//stride + 1\n",
    "    out_w = (w + 2*padding - filter_w)//stride + 1\n",
    "\n",
    "    X_flat = flatten(X, filter_h, filter_w, filter_c, out_h, out_w, stride, padding)\n",
    "    W_flat = tf.reshape(W, [filter_h*filter_w*filter_c, filter_n])\n",
    "    \n",
    "    z = tf.matmul(X_flat, W_flat) + b     # b: 1 X filter_n\n",
    "    \n",
    "    return tf.transpose(tf.reshape(z, [out_h, out_w, n, filter_n]), [2, 0, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KhIezV0giRmG"
   },
   "outputs": [],
   "source": [
    "def max_pool(X, pool_h, pool_w, padding, stride):\n",
    "    n, h, w, c = [d.value for d in X.get_shape()]\n",
    "    \n",
    "    out_h = (h + 2*padding - pool_h)//stride + 1\n",
    "    out_w = (w + 2*padding - pool_w)//stride + 1\n",
    "\n",
    "    X_flat = flatten(X, pool_h, pool_w, c, out_h, out_w, stride, padding)\n",
    "\n",
    "    pool = tf.reduce_max(tf.reshape(X_flat, [out_h, out_w, n, pool_h*pool_w, c]), axis=3)\n",
    "    return tf.transpose(pool, [2, 0, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3aMFgjd3ikWK"
   },
   "outputs": [],
   "source": [
    "def affine(X, W, b):\n",
    "    n = X.get_shape()[0].value # number of samples\n",
    "    X_flat = tf.reshape(X, [n, -1])\n",
    "    return tf.matmul(X_flat, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "2c8GyrG9Fp0H",
    "outputId": "97428328-716d-4a41-b54d-8ff39bf60d49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.856708665\n",
      "Epoch: 0002 cost= 0.120060328\n",
      "---\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## template\n",
    "with tf.Graph().as_default() as g:\n",
    "    \"\"\" Define dataset and iterator \"\"\"\n",
    "    with tf.name_scope(\"data\"):\n",
    "        X = tf.placeholder('float', [batch_size, 28, 28, 1])\n",
    "        t = tf.placeholder('float', [batch_size, 10])\n",
    "    \n",
    "    \"\"\" Build the model \"\"\"\n",
    "    with tf.name_scope(\"con1\"):\n",
    "        filter_h, filter_w, filter_c, filter_n = 5, 5, 1, 30\n",
    "        W1 = tf.Variable(tf.random_normal([filter_h, filter_w, filter_c, filter_n], stddev=0.01))\n",
    "        b1 = tf.Variable(tf.zeros([filter_n]))\n",
    "        conv_layer = convolution(X, W1, b1, padding=2, stride=1)\n",
    "        \n",
    "    with tf.name_scope(\"activation\"):\n",
    "        conv_activation_layer = Myrelu(conv_layer)\n",
    "    \n",
    "    with tf.name_scope(\"max_pool1\"):\n",
    "        pooling_layer = max_pool(conv_activation_layer, pool_h=2, pool_w=2, padding=0, stride=2)\n",
    "    \n",
    "    with tf.name_scope(\"fcn1\"):\n",
    "        batch_size, pool_output_h, pool_output_w, filter_n = [d.value for d in pooling_layer.get_shape()]\n",
    "        hidden_size = 100\n",
    "        W2 = tf.Variable(tf.random_normal([pool_output_h*pool_output_w*filter_n, hidden_size], stddev=0.01))\n",
    "        b2 = tf.Variable(tf.zeros([hidden_size]))\n",
    "        affine_layer1 = affine(pooling_layer, W2, b2)\n",
    "        affine_activation_layer1 = Myrelu(affine_layer1)\n",
    "    \n",
    "    with tf.name_scope(\"fcn2\"):\n",
    "        output_size = 10\n",
    "        W3 = tf.Variable(tf.random_normal([hidden_size, output_size], stddev=0.01))\n",
    "        b3 = tf.Variable(tf.zeros([output_size]))\n",
    "        affine_layer2 = affine(affine_activation_layer1, W3, b3)\n",
    "        softmax_layer = mySoftMax(affine_layer2)\n",
    "\n",
    "    \"\"\" Define the loss \"\"\"\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        loss = cross_entropy_error(softmax_layer, t)\n",
    "    \n",
    "    \"\"\" Define the optimizer \"\"\"\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    \"\"\" Other tensors or operations you need \"\"\"\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        t_predict = tf.argmax(softmax_layer, axis=1)\n",
    "        t_actual = tf.argmax(t, axis=1)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(t_predict, t_actual), tf.float32))\n",
    "        tf.summary.scalar('Accuracy', acc)\n",
    "        \n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    \"\"\" Initialize the variables \"\"\"\n",
    "    init = tf.global_variables_initializer()\n",
    "    \"\"\" Run the target tensors and operations \"\"\"\n",
    "    sess.run(init)\n",
    "    num_batch = int(mnist.train.num_examples/batch_size)\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        for _ in range(num_batch):\n",
    "            train_X, train_ys = mnist.train.next_batch(batch_size)\n",
    "            optimizer.run(feed_dict={X:train_X, t:train_ys})\n",
    "            avg_cost += loss.eval(feed_dict={X:train_X, t:train_ys}) / num_batch\n",
    "\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost), flush=True)\n",
    "\n",
    "    print(\"---\")\n",
    "    # 準確率\n",
    "    test_x = mnist.test.images[:batch_size]\n",
    "    test_t = mnist.test.labels[:batch_size]\n",
    "    print(\"Accuracy: \", sess.run(acc, feed_dict={X:test_x, t:test_t}))\n",
    "\n",
    "    # sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ekIscU474X0S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab10_104000033.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
